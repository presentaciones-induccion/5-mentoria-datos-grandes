---
# title: "Capacitación en R y herramientas de productividad"
# author: "Abril 2021"
format:
  revealjs:
    auto-stretch: false
    margin: 0
    slide-number: true
    scrollable: true
    preview-links: auto
    logo: imagenes/logo_portada2.png
    css: ine_quarto_styles.css
    # footer: <https://quarto.org>
---

#

<!---
# TODO: this does not work
 .linea-superior[]
.linea-inferior[] 
--->

<!---
# TODO: this does not work
 ![](imagenes/logo_portada2.png){.center style="width: 20%;"}   
--->

[]{.linea-superior} 
[]{.linea-inferior} 

<!---
 <img src="imagenes/logo_portada2.png" style="width: 20%"/>  
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  

[**Mentoría Grandes Volúmenes de Datos**]{.big-par .center-justified}

[**Proyecto Ciencia de Datos**]{.big-par .center-justified}

[**Octubre 2023**]{.big-par .center-justified}



## Tabla de contenidos 

<br>

::: {.incremental .big-par}
- **Problema y Desafíos asociados** <br><br>
- **Herramientas para datos grandes** <br><br>
- **Test realizados** <br><br>
- **Resultados** <br><br>
- **Conclusión** <br><br>
:::



## Problema y Desafíos

<!-- </br> -->

<p class="box-empty"><span > </span></p>

. . .

El desarrollo del Censo 2024 involucra la generación de un gran volumen de datos.

. . .

![](imagenes/table_persona_input.png){width="80%"}

. . .

<!-- <p class="box-empty-2"><span > </span></p> -->

![](imagenes/table_persona_variables_01.png){width="70%"}



## Problema y Desafíos

<p class="box-empty"><span > </span></p>

El desarrollo del Censo 2024 involucra la generación de un gran volumen de datos.

![](imagenes/table_persona_input.png){width="80%"}


![](imagenes/table_persona_variables_02.png){width="70%"}



## Problema y Desafíos

<p class="box-empty"><span > </span></p>

El desarrollo del Censo 2024 involucra la generación de un gran volumen de datos.

![](imagenes/table_persona_input.png){width="80%"}


![](imagenes/table_persona_variables_03.png){width="70%"}



## Problema y Desafíos

<p class="box-empty"><span > </span></p>

El desarrollo del Censo 2024 involucra la generación de un gran volumen de datos.

![](imagenes/table_persona_input.png){width="80%"}


![](imagenes/table_persona_variables_04.png){width="60%"}



## Problema y Desafíos

<p class="box-empty"><span > </span></p>

El desarrollo del Censo 2024 involucra la generación de un gran volumen de datos. <br><br>

. . .

Estos datos requieren ser revisados, mediante la creación de variables auxiliares que se utilizan para validar la información censada. <br><br>

. . .

Este proceso debe ser eficiente, confiable y rápido. Lo que conlleva varios desafíos:

::: {.incremental .medium-par}
- Cargar, procesar y escribir grandes volúmenes de datos (20M+ de registros) en el menor tiempo posible. <br>
- Procesar datos de manera confiable...**TODO** <br>
- Encontrar la herramienta apropiada para el tratamiento de este volumen de datos. <br>
- Evaluar diferentes escenarios de procesamiento, variando la herramienta usada, el volumen de los datos y la cantidad de variables auxiliares creadas...**TODO** <br>
:::



<!-- ## Problema y Desafíos

![](imagenes/table_persona_input.png){width="100%"}

![](imagenes/table_persona_variables_01.png){width="80%"}
 -->



## Herramientas para datos grandes

. . .

![](imagenes/logo_apache_arrow.png){.absolute top=150 left=100 width="170" height="150"}
![](imagenes/github_arrow_01.png){.absolute top=100 left=100 width="350" height="50"}
![](imagenes/github_arrow_02.png){.absolute top=150 left=300 width="150" height="200"}

. . .

![](imagenes/logo_apache_spark.png){.absolute top=150 left=600 width="170" height="150"}
![](imagenes/github_spark_01.png){.absolute top=100 left=600 width="350" height="50"}
![](imagenes/github_spark_02.png){.absolute top=150 left=800 width="150" height="200"}

. . .

![](imagenes/logo_duckdb.png){.absolute top=450 left=100 width="200" height="150"}
![](imagenes/github_duckdb_01.png){.absolute top=400 left=100 width="350" height="50"}
![](imagenes/github_duckdb_02.png){.absolute top=450 left=300 width="150" height="200"}

. . .

![](imagenes/logo_datatable.png){.absolute top=450 left=600 width="200" height="200"}
![](imagenes/github_datatable_01.png){.absolute top=400 left=600 width="350" height="50"}
![](imagenes/github_datatable_02.png){.absolute top=450 left=800 width="150" height="200"}


## Herramientas para datos grandes

**Apache Arrow:**

<!-- ![](imagenes/logo_apache_arrow.png){.absolute top=60 left=200 width="70" height="50"} -->

::: columns

:::{.column width="50%" .incremental .small-par}

- Su formato columnar *in-memory* y agnóstico al lenguaje permite representar datasets tabulares estructurados en memoria. <br><br>
- Lectura y escritura en muchos formatos comunes de almacenamiento (csv, parquet, feather, etc.). <br><br>
- Tiene implementaciones en muchos lenguajes (C++, Java, Python, R, etc.). <br><br>
- Implementación en R permite ejecutar operaciones analíticas o queries sobre objectos *Table* and *Dataset* con librería *dplyr*.  

:::

:::{.column width="50%" .small-par}

![](imagenes/arrow_columnar_format.png){width="100%"}

:::

:::


## Herramientas para datos grandes

**Apache Spark:**

<!-- ![](imagenes/logo_apache_spark.png){.absolute top=60 left=200 width="70" height="50"} -->

::: columns

:::{.column width="40%" .incremental .small-par}

- Framework para el procesamiento distribuido de grandes datos. <br><br>
- Procesamiento escala horizontalmente y se distribuye en múltiples nodos de un cluster. <br><br>
- Procesos Spark: 
  - Coordinados por el objeto *SparkContext* en nuestro programa main (driver).
  - *SparkContext* se conecta a un *Cluster Manager* (Standalone, YARN, Kubernetes) que asigna los recursos del cluster.

:::

:::{.column width="60%" .incremental .small-par}

![](imagenes/spark_02_1.png){width="100%"}

![](imagenes/spark_01.png){width="70%"}

:::

:::


## Herramientas para datos grandes

**Apache Spark:**

<!-- ![](imagenes/logo_apache_spark.png){.absolute top=60 left=200 width="70" height="50"} -->

::: columns

:::{.column width="40%" .small-par}

- Framework para el procesamiento distribuido de grandes datos. <br><br>
- Procesamiento escala horizontalmente y se distribuye en múltiples nodos de un cluster. <br><br>
- Procesos Spark: 
  - Coordinados por el objeto *SparkContext* en nuestro programa main (driver).
  - *SparkContext* se conecta a un *Cluster Manager* (Standalone, YARN, Kubernetes) que asigna los recursos del cluster.

:::

:::{.column width="60%" .incremental .small-par}

![](imagenes/spark_02_2.png){width="100%"}

![](imagenes/spark_01.png){width="70%"}

:::

:::


## Herramientas para datos grandes

**Apache Spark:**

<!-- ![](imagenes/logo_apache_spark.png){.absolute top=60 left=200 width="70" height="50"} -->

::: columns

:::{.column width="40%" .small-par}

- Framework para el procesamiento distribuido de grandes datos. <br><br>
- Procesamiento escala horizontalmente y se distribuye en múltiples nodos de un cluster. <br><br>
- Procesos Spark: 
  - Coordinados por el objeto *SparkContext* en nuestro programa main (driver).
  - *SparkContext* se conecta a un *Cluster Manager* (Standalone, YARN, Kubernetes) que asigna los recursos del cluster.

:::

:::{.column width="60%" .incremental .small-par}

![](imagenes/spark_02_3.png){width="100%"}

![](imagenes/spark_01.png){width="70%"}

:::

:::


## Herramientas para datos grandes

**Apache Spark:**

<!-- ![](imagenes/logo_apache_spark.png){.absolute top=60 left=200 width="70" height="50"} -->

::: columns

:::{.column width="40%" .small-par}

- Framework para el procesamiento distribuido de grandes datos. <br><br>
- Procesamiento escala horizontalmente y se distribuye en múltiples nodos de un cluster. <br><br>
- Procesos Spark: 
  - Coordinados por el objeto *SparkContext* en nuestro programa main (driver).
  - *SparkContext* se conecta a un *Cluster Manager* (Standalone, YARN, Kubernetes) que asigna los recursos del cluster.

:::

:::{.column width="60%" .incremental .small-par}

![](imagenes/spark_02_4.png){width="100%"}

![](imagenes/spark_01.png){width="70%"}

:::

:::


## Herramientas para datos grandes

**Apache Spark:**

<!-- ![](imagenes/logo_apache_spark.png){.absolute top=60 left=200 width="70" height="50"} -->

::: columns

:::{.column width="40%" .incremental .small-par}

- Procesamiento versátil mediante varias librerías: en batches (Spark Core), queries SQL (Spark SQL), streaming (Spark Streaming), en graphs (GraphX) y machine learning (MLlib). <br><br>
- APIs de alto nivel para varios lenguages: Scala, Java, Python y R. <br><br>
- Integrable con diversas fuentes de datos: Hadoop Distributed File System (HDFS), BBDD relacionales, data lakes, etc. <br><br>
- Soporta varios formatos de archivo: CSV, Parquet, Avro, JSON, etc. <br><br>

:::

:::{.column width="60%" .incremental .small-par}

![](imagenes/spark_ecosystem_03.png){width="100%"}

:::

:::



## Herramientas para datos grandes 

**Duck DB:**

<!-- ![](imagenes/logo_duckdb.png){.absolute top=65 left=140 width="80" height="40"} -->

::: {.incremental .medium-par}
- Sistema de manejo de base de datos analíticas (DBMS) diseñado para reportería y analítica. <br><br>
- Almacena data en formato columnar, que está optimizado para queries analíticas. <br><br>
- Usa un motor de ejecución de queries vectorizado, que procesa múltiple elementos de data (vectores) en una única instrucción. <br><br> 
- Soporta sintaxis SQL estándar. <br><br>
- Escrito en C++, no tiene dependencias. 
:::


## Herramientas para datos grandes 

**data.table:**

<!-- ![](imagenes/logo_datatable.png){.absolute top=60 left=140 width="80" height="60"} -->

::: {.incremental .medium-par}
- Paquete de R que ofrece una versión de alto rendimiento del *data.frame* (estructure estándar para almacenar data en R). <br><br>
- Internamente muchas operaciones comunes son paralelizadas para usar múltiples hilos de CPU. <br><br>
- No tiene otras dependencias que R base. <br><br>
- Ampliamente usado en la comunidad R y directamente usado por cientos de paquetes CRAN. <br><br>
:::


<!-- ## Caso práctico: Censo

TODO:

::: {.medium-par}
- Describe dataset. <br><br>
- Describe different tests. <br><br>
- Describe different processing stages. <br><br>
- Describe different tools used. <br><br>
::: -->


<!-- ## Tests: Herramienta de procesamiento

::: {.medium-par}

<br><br>

```{python}
#| label: tbl-planet-measures

from IPython.display import Markdown
from tabulate import tabulate
table = [["Sin aumentar ","Dataset original"],
         ["Aumentado +5M ","Dataset con 5M de registros repetidos"],
         ["Aumentado +10M. Variables 2X","Dataset con 10M de registros repetidos"]
        ]
Markdown(tabulate(
  table, 
  headers=["Volumen","Detalle"]
))
```

::: -->



## Tests: Herramienta de procesamiento

<!-- <br> -->

. . .

<p class="box-empty"><span > </span></p>

Evaluamos diferentes escenarios de procesamiento: variando la herramienta usada, el volumen de los datos y la cantidad de variables auxiliares creadas, en diferentes etapas. 



## Tests: Herramienta de procesamiento

<!-- <br> -->

<p class="box-empty"><span > </span></p>

[Evaluamos diferentes escenarios de procesamiento: variando la]{.lightgray} **herramienta usada** [, el volumen de los datos y la cantidad de variables auxiliares creadas, en diferentes etapas.]{.lightgray} 

. . .

::: {.medium-par}

<!-- <br> -->

<p class="box-empty"><span > </span></p>
<!-- [""]{.box} -->

+----------------------+------------------------------------------------------------------+
| Herramienta          | Detalle                                                          |
+======================+==================================================================+
| Apache Arrow (R)     | Implementación de Arrow en R (analítica usando *dplyr*)          |
|                      |                                                                  |
+----------------------+------------------------------------------------------------------+
| Apache Arrow         | Implementación de Arrow en R (analítica usando *dplyr*). <br>    |
| + DuckDB (R)         | Algunas operaciones ejecutadas usando DuckDB                     |
+----------------------+------------------------------------------------------------------+
| Apache Arrow         | Implementación de Arrow en Python (PyArrow). <br>                |
| + (Python)           | Analítica usando sintaxis de Arrow.                              |
+----------------------+------------------------------------------------------------------+
| data.table (R)       | Librería data.table de R                                         |
|                      |                                                                  |
+----------------------+------------------------------------------------------------------+
| Apache Spark (R)     | Implementación de Spark en R (librería *sparklyr*). <br>         |
|                      | Usando 1 cluster (máquina local)                                 |
+----------------------+------------------------------------------------------------------+

::: 



## Tests: Volumen de datos

<!-- . . . -->

<p class="box-empty"><span > </span></p>

[Evaluamos diferentes escenarios de procesamiento: variando la herramienta usada, el]{.lightgray} **volumen de los datos y la cantidad de variables auxiliares creadas** [en diferentes etapas]{.lightgray}. 

. . .

::: {.medium-par}

<p class="box-empty"><span > </span></p>

+----------------+---------------------------------------------+
| Volumen        | Detalle                                     |
+================+=============================================+
| Sin aumentar   | Dataset original                            |
|                |                                             |
+----------------+---------------------------------------------+
| Aumentado +5M  | Dataset con 5M de registros repetidos       |
|                |                                             |
+----------------+---------------------------------------------+
| Aumentado +10M | Dataset con 10M de registros repetidos      |
|                |                                             |
+----------------+---------------------------------------------+
| Sin aumentar   | Dataset con el doble de variables creadas   |
| Variables 2X   |                                             |
+----------------+---------------------------------------------+
| Aumentado +5M  | Dataset con 5M de registros repetidos       |
| Variables 2X   | y con el doble de variables creadas         |
+----------------+---------------------------------------------+
| Aumentado 10M  | Dataset con 10M de registros repetidos      |
| Variables 2X   | y con el doble de variables creadas         |
+----------------+---------------------------------------------+

::: 


## Tests: Etapa en procesamiento

<!-- . . . -->

<p class="box-empty"><span > </span></p>

[Evaluamos diferentes escenarios de procesamiento: variando la herramienta usada, el volumen de los datos y la cantidad de variables auxiliares creadas, en]{.lightgray} **diferentes etapas.** 

. . .

::: {.medium-par}

<p class="box-empty"><span > </span></p>

+----------------------+--------------------------------------------------------------------------+
| Etapa                | Detalle                                                                  |
+======================+==========================================================================+
| Cargar data          | Lectura de dataset en R o Python                                         |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Homologar variables  | Transformación de algunas columnas en dataset                            |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Crear indices        | Creación de índices para columnas de agrupación (sólo para Data table)   |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Crear variables      | Creación de variables auxiliares de validación                           |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Merge variables      | Consolidación de variables de validación creadas                         |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Left join            | Unión de variables de validación a dataset                               |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Exportar data        | Guardado de dataset transformado                                         |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+

:::



## Código procesamiento: Cargar data

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.feather"
persona <- arrow::read_feather(
  filename,
  as_data_frame = F,
  col_select=columnas_selected
)

cols_partition <- c("REGION","PROVINCIA","COMUNA")
format_dataset <- "feather"

path_dataset <- paste0(ubi,"test_arrow_dataset_in_tmp")
arrow::write_dataset(persona, path_dataset, format=format_dataset, partitioning=cols_partition)
persona <- arrow::open_dataset(path_dataset, format=format_dataset, partitioning=cols_partition)
```

### [Apache Arrow (Python)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE

filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.feather"
persona = ft.read_table(filename) # result is pyarrow.Table
# persona = ft.read_feather(filename) # result is pandas.DataFrame

# create dataset
path_dataset = f"{ubi}test_pyarrow_dataset_in_tmp"
partitioning = ["REGION","PROVINCIA"]
ds.write_dataset(
    persona,
    base_dir=path_dataset,
    format="feather",
    partitioning=partitioning,
    existing_data_behavior="delete_matching"
)
```

### [Data Table (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.csv"
persona <- data.table::fread(
  filename,
  select=list(integer=columnas_integer, character=columnas_character)
)
```

### [Apache Spark (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.feather"
persona <- spark_read_csv(
  sc,
  name="persona",
  path=filename,
  header=TRUE,
  columns=columnas,
  infer_schema=FALSE,
  delimiter=';',
  repartition=0,
  memory=FALSE
) %>%
select(all_of(columnas_2))
```

:::


## Código procesamiento: Homologar variables

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.small-par} 

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- persona %>%
  mutate(
    entrevista_id = as.integer64(paste0(REGION,PROVINCIA,COMUNA,DC,AREA,ZC_LOC,NVIV)),
    rst_hogares__id = as.integer64(NHOGAR),
    parentesco = as.integer64(P07),
    sexo = as.integer64(P08),
    sexo_validado = as.integer64(P08),
    edad = as.integer64(P09),
    edad_validada = as.integer64(P09),
    ...
  ) %>%
  compute()
```

### [Apache Arrow (Python)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
# this is valid when persona is pyarrow.lib.Table
col_names = persona.column_names

col_names_new = []
for i, c in enumerate(col_names):
    if c == "NHOGAR":
        c_new = "rst_hogares__id"
    elif c == "P07":
        c_new = "parentesco"
    elif c == "P08":
        c_new = "sexo"
    elif c == "P09":
        c_new = "edad"
    ...
    else:
        c_new = c
    col_names_new.append(c_new)
persona = persona.rename_columns(col_names_new)

persona = persona.append_column("edad_validada", persona["edad"])
persona = persona.append_column("sexo_validado", persona["sexo"])

entrevista_id = pc.binary_join_element_wise(
    pc.cast(persona['REGION'], pa.string()),
    pc.cast(persona['COMUNA'], pa.string()),
    pc.cast(persona['DC'], pa.string()),
    pc.cast(persona['AREA'], pa.string()),
    pc.cast(persona['ZC_LOC'], pa.string()),
    pc.cast(persona['NVIV'], pa.string()),    
    ""
)
persona = persona.append_column("entrevista_id", entrevista_id)
```

### [Data Table (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
columnas_integer <- c(
  "NHOGAR",
  "P07",
  "P08",
  "P09",
  ...
)
columnas_new <- c(
  "rst_hogares__id",
  "parentesco",
  "sexo",
  "edad",
  ...
)
setnames(persona, columnas_integer, columnas_new, skip_absent=TRUE) # skips columns which are not par of df

# new columns 
persona[, ':='(
  entrevista_id = as.double(paste0(REGION,PROVINCIA,COMUNA,DC,AREA,ZC_LOC,NVIV)),
  sexo_validado = sexo,
  edad_validada = edad
)]
```

### [Apache Spark (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- persona %>%
  mutate(
    entrevista_id = as.double(paste0(REGION,PROVINCIA,COMUNA,DC,AREA,ZC_LOC,NVIV)),
    sexo_validado = sexo,
    edad_validada = edad
  )
```

:::



## Código procesamiento: Crear índices

::: {.panel-tabset}

### [Data Table (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
setindex(persona, entrevista_id, rst_hogares__id, parentesco, sexo_validado, edad_validada)
```

:::



## Código procesamiento: Crear variables

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.small-par} 

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- persona %>% 
  mutate(edad_edu_curso = edad_validada - edu_curso) %>%
  compute()

# base persona nivel hogar
pers_niv_hog <- persona %>% 
  group_by(entrevista_id, rst_hogares__id) %>%
  summarize(cant_per_hog = n()) %>%
  compute()

# edad del jefe del hogar
edad_jefe_hogar <- persona %>%
  filter(parentesco==1) %>%
  group_by(entrevista_id,rst_hogares__id) %>%
  summarize(edad_jh = max(edad_validada),
            sexo_jh = max(sexo_validado)) %>%
  compute()
```

### [Apache Arrow (Python)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
edad_edu_curso = pc.subtract(persona["edad_validada"],persona["edu_curso"])
persona = persona.append_column("edad_edu_curso", edad_edu_curso)
del edad_edu_curso

# base persona nivel hogar
pers_niv_hog = persona\
    .group_by(["entrevista_id","rst_hogares__id"])\
    .aggregate([("entrevista_id", "count")])\
    .rename_columns(["entrevista_id","rst_hogares__id","cant_per_hog"])

# edad del jefe del hogar
edad_jefe_hogar = persona\
    .filter(pc.field("parentesco") == 1)\
    .group_by(["entrevista_id","rst_hogares__id"])\
    .aggregate([("edad_validada", "max"), ("sexo_validado", "max")])\
    .rename_columns(["entrevista_id","rst_hogares__id","edad_jh","sexo_jh"])
```

### [Data Table (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
persona[, edad_edu_curso := edad_validada - edu_curso]

# base persona nivel hogar
pers_niv_hog <- persona[
  ,
  .(cant_per_hog = .N),
  by = c("entrevista_id","rst_hogares__id")
]

# edad del jefe del hogar
edad_jefe_hogar <- persona[
  parentesco==1,
  .(edad_jh = max(edad_validada), sexo_jh = max(sexo_validado)),
  by = c("entrevista_id","rst_hogares__id")
]
```

### [Apache Spark (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- persona %>%
  mutate(edad_edu_curso = edad_validada - edu_curso)

# base persona nivel hogar
pers_niv_hog <- persona %>%
  group_by(entrevista_id, rst_hogares__id) %>%
  summarize(cant_per_hog = n(), .groups = 'drop')

# edad del jefe del hogar
edad_jefe_hogar <- persona %>%
  filter(parentesco==1) %>%
  group_by(entrevista_id,rst_hogares__id) %>%
  summarize(edad_jh = max(edad_validada),
            sexo_jh = max(sexo_validado), .groups = 'drop')
```

:::



## Código procesamiento: Merge variables

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.small-par} 

```{r}
#| echo: TRUE
#| eval: FALSE
# union de edades construidas a nivel hogar
edad_hhpmss_jefe_hogar <- pers_niv_hog %>%
  left_join(edad_jefe_hogar, by = c("entrevista_id","rst_hogares__id")) %>%
  compute()

edad_hhpmss_jefe_hogar <- edad_hhpmss_jefe_hogar %>%
  left_join(edad_hno_min_max_jefe_hogar, by = c("entrevista_id","rst_hogares__id")) %>%
  compute()
```

### [Apache Arrow (Python)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
ks = ["entrevista_id","rst_hogares__id"]

edad_hhpmss_jefe_hogar = pers_niv_hog\
    .join(edad_jefe_hogar, keys=ks)\
    .join(edad_hno_min_max_jefe_hogar, keys=ks)\
    ...
```

### [Data Table (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
edad_hhpmss_jefe_hogar <- merge(pers_niv_hog, edad_jefe_hogar, by=c("entrevista_id","rst_hogares__id"), all.x=TRUE, all.y=FALSE)
edad_hhpmss_jefe_hogar <- merge(edad_hhpmss_jefe_hogar, edad_hno_min_max_jefe_hogar, by=c("entrevista_id","rst_hogares__id"), all.x=TRUE, all.y=FALSE)
```

### [Apache Spark (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
edad_hhpmss_jefe_hogar <- pers_niv_hog %>%
  left_join(edad_jefe_hogar, by = c("entrevista_id","rst_hogares__id")) %>%
  left_join(edad_hno_min_max_jefe_hogar, by = c("entrevista_id","rst_hogares__id")) %>%
  ...
```

:::


## Código procesamiento: Left join

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.small-par} 

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- persona %>%
  left_join(edad_hhpmss_jefe_hogar, by = c("entrevista_id","rst_hogares__id")) %>%
  compute()
```

### [Apache Arrow (Python)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
ks = ["entrevista_id","rst_hogares__id"]

persona = persona\
    .join(edad_hhpmss_jefe_hogar, keys=ks)
```

### [Data Table (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- merge(
  persona,
  edad_hhpmss_jefe_hogar,
  by=c("entrevista_id","rst_hogares__id"),
  all.x=TRUE,
  all.y=FALSE
)
```

### [Apache Spark (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- persona %>%
  left_join(edad_hhpmss_jefe_hogar, by = c("entrevista_id","rst_hogares__id"))
```

:::


## Código procesamiento: Exportar data

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
format_dataset <- "feather"
path_dataset <- "test_arrow_dataset_out"

arrow::write_dataset(persona, path_dataset, format=format_dataset, partitioning=cols_partition)
```

### [Apache Arrow (Python)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE

partitioning = ["REGION","PROVINCIA"]
path_dataset = "test_pyarrow_dataset_out"

ds.write_dataset(
    persona,
    base_dir=path_dataset,
    format="feather",
    partitioning=partitioning,
    existing_data_behavior="delete_matching"
)
```

### [Data Table (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename_out <- "integrada_persona____data_no_aumentado.feather"

feather::write_feather(persona, filename_out)
```

### [Apache Spark (R)]{.small-par}

```{r}
#| echo: TRUE
#| eval: FALSE
dir_out <- "integrada_persona_csv__data_no_aumentado"

sparklyr::spark_write_csv(
  persona,
  path=dir_out,
  header = TRUE,
  delimiter = ";",
  quote = "\"",
  escape = "\\",
  charset = "UTF-8",
  null_value = NULL,
  options = list(),
  mode = "overwrite"
)
sparklyr::spark_disconnect(sc)
```

:::



<!-- ## Resultados

::: {.panel-tabset}

### [1]{.small-par}

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_no_aum.png" width="100%"/>  

### [2]{.small-par}

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_05M.png" width="100%"/>  

### [3]{.small-par}

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_10M.png" width="100%"/>  


### [4]{.small-par}

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_no_aum_var_aum1X.png" width="100%"/>  

### [5]{.small-par}

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_05M_var_aum1X.png" width="100%"/>  


### [6]{.small-par}

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_10M_var_aum1X.png" width="100%"/>  

::: -->


## Resultados: Data sin aumentar

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_no_aum.png" width="100%"/>  


## Resultados: Data aumentado +5M

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_05M.png" width="100%"/>  


## Resultados: Data aumentado +10M

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_10M.png" width="100%"/>  


## Resultados: Data sin aumentar + variables 2X

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_no_aum_var_aum1X.png" width="100%"/>  


## Resultados: Data aumentado +5M + variables 2X

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_05M_var_aum1X.png" width="100%"/>  


## Resultados: Data aumentado +10M + variables 2X

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_10M_var_aum1X.png" width="100%"/>  


## Resultados: Todos los tests

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_test_1.png" width="110%"/>  



## Conclusión

::: {.incremental .medium-par}

- La implementación de *Spark* usada no escala eficientemente ni al aumentar los datos ni al incrementar el número de variables.
  - Los efectos de usar *Spark* debieran ser más notorios al usar un cluster más poderoso: varias máquinas con más RAM, comparado con 1 cluster (máquina local) usado para este test.

- El procesamiento con *Arrow + DuckDB* es más rápido que con *data.table* para los tests menos exigentes, lo que se invierte para los test más exigentes (con mayor aumento de datos o de variables calculadas).
  - Esto demuestra que *DuckDB* no escala eficientemente, e incluso es más lento que *Spark*, para las queries más exigentes.

- Los procesamientos más rápidos se obtienen al usar las implementaciones de *Arrow* en R y Python.
  - Siendo más rápida la de R por una diferencia que se incrementa para los tests más exigentes (con más datos y más variables calculadas).
  - Esto puede deberse a que Arrow en R usa *dplyr*, mientras que en Python se usó sintaxis nativa de *Arrow*...**TODO**

:::


## Referencias

**TODO**: Agregar referencias a documentación de herramientas / algoritmos

::: {.medium-par}

- [R for Data Science, de Hadley Wickham](https://r4ds.had.co.nz/)

:::

#

<!---
# TODO: this does not work
 .linea-superior[]
.linea-inferior[] 
--->

<!---
# TODO: this does not work
 ![](imagenes/logo_portada2.png){.center style="width: 20%;"}   
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  

[**Mentoría Grandes Volúmenes de Datos**]{.big-par .center-justified}

[**Proyecto Ciencia de Datos**]{.big-par .center-justified}

[**Octubre 2023**]{.big-par .center-justified}

[]{.linea-superior} 
[]{.linea-inferior} 
