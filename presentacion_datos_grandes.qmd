---
# title: "Capacitación en R y herramientas de productividad"
# author: "Abril 2021"
format:
  revealjs:
    auto-stretch: false
    margin: 0
    slide-number: true
    scrollable: true
    preview-links: auto
    logo: imagenes/logo_portada2.png
    css: ine_quarto_styles.css
    # footer: <https://quarto.org>
---

#

<!---
# TODO: this does not work
 .linea-superior[]
.linea-inferior[] 
--->

<!---
# TODO: this does not work
 ![](imagenes/logo_portada2.png){.center style="width: 20%;"}   
--->

[]{.linea-superior} 
[]{.linea-inferior} 

<!---
 <img src="imagenes/logo_portada2.png" style="width: 20%"/>  
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  

[**Mentoría Grandes Volúmenes de Datos**]{.big-par .center-justified}

[**Proyecto Estratégico Servicios Compartidos para la Producción Estadística**]{.big-par .center-justified}

[**Inducción - Septiembre 2023**]{.big-par .center-justified}


## Grandes Volúmenes de Datos 

. . .

Esta actividad de capacitación se enmarca en el **PE SSCC**...

. . .

El PE SSCC es uno de los 4 proyectos estratégicos actualmente en funcionamiento en el INE (2018-2022)...

. . .

Este proyecto busca:

. . .


[*"Proveer a la institución de estándares y desarrollos que permitan [automatizar]{.red}, [estandarizar]{.red}, [ordenar]{.red}, e [innovar]{.red} en la producción estadística, permitiendo reducir [tiempos]{.red} y [costos]{.red} del procesamiento y análisis de las diferentes operaciones estadísticas del INE y minimizando la probabilidad de [errores]{.red} en la publicación de resultados".*]{.medium-par}


## Objetivos del proyecto 

</br>

. . .

Al finalizar la capacitación se espera que las/os participantes:

::: {.incremental .medium-par}
- **Estén familiarizados con el lenguaje de programación R** y cuenten con las habilidades necesarias para **profundizar aspectos de su interés** en este lenguaje. <br><br>
- Sean capaces de **explorar** y **transformar** objetos en R. <br><br>
- Manejen herramientas de **visualización** en R. <br><br>
- Aprendan **buenas prácticas de escritura de código** en R. <br><br>
- Conozcan **recomendaciones** que les permitan hacer **reproducibles** y **trazables** sus rutinas en R.
:::

## Contenidos de la presentación 

. . .

</br>

**Sesión 1:** Introducción y herramientas de exploración de datos

    - ¿Qué es R y por qué usarlo?
    - La interfaz de R Studio
    - Tipos de datos y operaciones básicas
    - Exploración y manipulación básica de un data frame

. . .

</br>

**Sesión 2:** Procesamiento de bases de datos (1)

    - Importación de datos desde distintos formatos a R
    - Introducción a dplyr
    - Manipulación básica de columnas (select(), rename())
    - Manipulación básica de filas (arrange(), filter())
    - Herramientas básicas de edición de datos (if_else())


## ¿Volúmenes grandes de datos? 

. . .

<!---
 ![](https://github.com/kjhealy/england_wales_pop/raw/master/figures/eng-wa-pop-pyr-labs-opt.gif){.absolute top="100" left="30" width="500" height="550"} 
--->

<!---
 . . . 
--->

<!---
 ![](https://revolution-computing.typepad.com/.a/6a010534b1db25970b01b8d1a4d670970c-pi){.absolute .fragment top="100" right="80" width="450"} 
--->


<!---
 [<img src="https://github.com/kjhealy/england_wales_pop/raw/master/figures/eng-wa-pop-pyr-labs-opt.gif" width="70%"/>]{.pull-left}  
--->

[<img src="https://github.com/kjhealy/england_wales_pop/raw/master/figures/eng-wa-pop-pyr-labs-opt.gif" width="70%"/>]{.absolute top="100" left="70" width="500" height="550"}

<!---
 [<img src="https://revolution-computing.typepad.com/.a/6a010534b1db25970b01b8d1a4d670970c-pi" height="300" width="400"/>]{.pull-right}  
--->

[<img src="https://revolution-computing.typepad.com/.a/6a010534b1db25970b01b8d1a4d670970c-pi" height="300" width="400"/>]{.absolute .fragment top="100" right="60" width="450"} 


## Herramientas para grandes volúmenes de datos

![](imagenes/logo_apache_arrow.png){.absolute top=150 left=300 width="200" height="200"}

![](imagenes/logo_apache_spark.png){.absolute top=150 left=600 width="200" height="200"}

![](imagenes/logo_datatable.png){.absolute top=400 left=300 width="200" height="200"}

![](imagenes/logo_duckdb.png){.absolute top=425 left=600 width="200" height="150"}


## Herramientas para grandes volúmenes de datos 

**Apache Spark:**

::: {.medium-par}
- Architecture. <br><br>
- Main characteristics. <br><br>
:::


## Herramientas para grandes volúmenes de datos 

**Apache Arrow:**

::: {.medium-par}
- Architecture. <br><br>
- Main characteristics. <br><br>
:::


## Herramientas para grandes volúmenes de datos 

**Data Table:**

::: {.medium-par}
- Architecture. <br><br>
- Main characteristics. <br><br>
:::


## Herramientas para grandes volúmenes de datos 

**Duck DB:**

::: {.medium-par}
- Architecture. <br><br>
- Main characteristics. <br><br>
:::


## Caso práctico: Censo

TODO:

::: {.medium-par}
- Describe dataset. <br><br>
- Describe different tests. <br><br>
- Describe different processing stages. <br><br>
- Describe different tools used. <br><br>
:::


## Caso práctico: Censo - Tests

::: {.medium-par}

+----------------+
| Test           |
+================+
| Sin aumentar   |
|                |
+----------------+
| Aumentado +5M  |
|                |
+----------------+
| Aumentado +10M |
|                |
+----------------+
| Sin aumentar   |
| Variables 2X   |
+----------------+
| Aumentado +5M  |
| Variables 2X   |
+----------------+
| Aumentado 10M  |
| Variables 2X   |
+----------------+

::: 


## Caso práctico: Censo - Etapas en procesamiento

::: {.medium-par}

+----------------------+
| Etapa                |
+======================+
| Cargar data          |
|                      |
+----------------------+
| Homologar variables  |
|                      |
+----------------------+
| Crear indices        |
|                      |
+----------------------+
| Crear variables      |
|                      |
+----------------------+
| Merguear variables   |
|                      |
+----------------------+
| Left join            |
|                      |
+----------------------+
| Exportar data        |
|                      |
+----------------------+

::: 


## Caso práctico: Censo - Herramientas

::: {.medium-par}

+----------------------+
| Algoritmo            |
+======================+
| Apache Arrow (R)     |
|                      |
+----------------------+
| Apache Arrow         |
| + DuckDB (R)         |
+----------------------+
| Apache Arrow         |
| + (Python)           |
+----------------------+
| Data Table (R)       |
|                      |
+----------------------+
| Apache Spark (R)     |
|                      |
+----------------------+

::: 


## Algoritmos: Cargar data

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.feather"
persona <- arrow::read_feather(
  filename,
  as_data_frame = F,
  col_select=columnas_selected
)

cols_partition <- c("REGION","PROVINCIA","COMUNA")
format_dataset <- "feather"

path_dataset <- paste0(ubi,"test_arrow_dataset_in_tmp")
arrow::write_dataset(persona, path_dataset, format=format_dataset, partitioning=cols_partition)
persona <- arrow::open_dataset(path_dataset, format=format_dataset, partitioning=cols_partition)
```

### [Apache Arrow (Python)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE

filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.feather"
persona = ft.read_table(filename) # result is pyarrow.Table
# persona = ft.read_feather(filename) # result is pandas.DataFrame

# create dataset
path_dataset = f"{ubi}test_pyarrow_dataset_in_tmp"
partitioning = ["REGION","PROVINCIA"]
ds.write_dataset(
    persona,
    base_dir=path_dataset,
    format="feather",
    partitioning=partitioning,
    existing_data_behavior="delete_matching"
)
```

### [Data Table (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.csv"
persona <- data.table::fread(
  filename,
  select=list(integer=columnas_integer, character=columnas_character)
)
```

### [Apache Spark (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.feather"
persona <- spark_read_csv(
  sc,
  name="persona",
  path=filename,
  header=TRUE,
  columns=columnas,
  infer_schema=FALSE,
  delimiter=';',
  repartition=0,
  memory=FALSE
) %>%
select(all_of(columnas_2))
```

:::


## Algoritmos: Homologar variables

::: {.medium-par}
- Code example. <br><br>
:::


## Algoritmos: Crear índices

::: {.medium-par}
- Code example. <br><br>
:::


## Algoritmos: Crear variables

::: {.medium-par}
- Code example. <br><br>
:::


## Algoritmos: Merge variables

::: {.medium-par}
- Code example. <br><br>
:::


## Algoritmos: Left join

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.medium-par} 

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- persona %>%
  left_join(edad_hhpmss_jefe_hogar, by = c("entrevista_id","rst_hogares__id")) %>%
  compute()
```

### [Apache Arrow (Python)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
ks = ["entrevista_id","rst_hogares__id"]

persona = persona\
    .join(edad_hhpmss_jefe_hogar, keys=ks)
```

### [Data Table (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- merge(
  persona,
  edad_hhpmss_jefe_hogar,
  by=c("entrevista_id","rst_hogares__id"),
  all.x=TRUE,
  all.y=FALSE
)
```

### [Apache Spark (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- persona %>%
  left_join(edad_hhpmss_jefe_hogar, by = c("entrevista_id","rst_hogares__id"))
```

:::


## Algoritmos: Exportar data

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
format_dataset <- "feather"
path_dataset <- "test_arrow_dataset_out"

arrow::write_dataset(persona, path_dataset, format=format_dataset, partitioning=cols_partition)
```

### [Apache Arrow (Python)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE

partitioning = ["REGION","PROVINCIA"]
path_dataset = "test_pyarrow_dataset_out"

ds.write_dataset(
    persona,
    base_dir=path_dataset,
    format="feather",
    partitioning=partitioning,
    existing_data_behavior="delete_matching"
)
```

### [Data Table (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename_out <- "integrada_persona____data_no_aumentado.feather"

feather::write_feather(persona, filename_out)
```

### [Apache Spark (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
dir_out <- "integrada_persona_csv__data_no_aumentado"

sparklyr::spark_write_csv(
  persona,
  path=dir_out,
  header = TRUE,
  delimiter = ";",
  quote = "\"",
  escape = "\\",
  charset = "UTF-8",
  null_value = NULL,
  options = list(),
  mode = "overwrite"
)
sparklyr::spark_disconnect(sc)
```

:::


## Resultados: Data sin aumentar

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_no_aum.png" width="100%"/>  


## Resultados: Data aumentado +5M

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_05M.png" width="100%"/>  


## Resultados: Data aumentado +10M

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_10M.png" width="100%"/>  


## Resultados: Data sin aumentar + variables 2X

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_no_aum_var_aum1X.png" width="100%"/>  


## Resultados: Data aumentado +5M + variables 2X

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_05M_var_aum1X.png" width="100%"/>  


## Resultados: Data aumentado +10M + variables 2X

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_10M_var_aum1X.png" width="100%"/>  



## Referencias

TODO: Agregar referencias a documentación de herramientas / algoritmos

::: {.medium-par}

- [R for Data Science, de Hadley Wickham](https://r4ds.had.co.nz/)

:::

#

<!---
# TODO: this does not work
 .linea-superior[]
.linea-inferior[] 
--->

<!---
# TODO: this does not work
 ![](imagenes/logo_portada2.png){.center style="width: 20%;"}   
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  

[**Grandes Volúmenes de Datos**]{.big-par .center-justified}

[**Proyecto Estratégico Servicios Compartidos para la Producción Estadística**]{.big-par .center-justified}

[**Inducción - Septiembre 2023**]{.big-par .center-justified}

[]{.linea-superior} 
[]{.linea-inferior} 

<!---
 ## Polar Axis  

For a demonstration of a line plot on a polar axis, see @fig-polar.

```{python}
#| label: fig-polar
#| fig-cap: "A line plot on a polar axis, not a bear"

import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 4 * np.pi * r
fig, ax = plt.subplots(
  subplot_kw = {'projection': 'polar'} 
)
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
``` 
--->