---
# title: "Capacitación en R y herramientas de productividad"
# author: "Abril 2021"
format:
  revealjs:
    auto-stretch: false
    margin: 0
    slide-number: true
    scrollable: true
    preview-links: auto
    logo: imagenes/logo_portada2.png
    css: ine_quarto_styles.css
    # footer: <https://quarto.org>
---

#

<!---
# TODO: this does not work
 .linea-superior[]
.linea-inferior[] 
--->

<!---
# TODO: this does not work
 ![](imagenes/logo_portada2.png){.center style="width: 20%;"}   
--->

[]{.linea-superior} 
[]{.linea-inferior} 

<!---
 <img src="imagenes/logo_portada2.png" style="width: 20%"/>  
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  

[**Mentoría Grandes Volúmenes de Datos**]{.big-par .center-justified}

[**Proyecto Ciencia de Datos**]{.big-par .center-justified}

[**Octubre 2023**]{.big-par .center-justified}


## Tabla de contenidos 

. . .

</br>

**Problema y Desafíos asociados**

. . .

</br>

**Herramientas para datos grandes**

. . .

</br>

**Test realizados**

. . .

</br>

**Resultados tests**

. . .

</br>

**Conclusión**

. . .


## Problema y Desafíos

</br>

. . .

Como parte del Censo 2024, es necesaria la creación de variables auxiliares para la validación de los datos.

. . .

Este proceso envuelve un gran volumen de datos, debe ser eficiente y rápido. Lo que conlleva varios desafíos:

::: {.incremental .medium-par}
- Cargar, procesar y escribir grandes volúmenes de datos (20M+ de registros) en el menor tiempo posible. <br>
- Procesar datos de manera confiable... <br>
- Explorar diferentes herramientas disponibles para el tratamiento de grandes volúmenes de datos. <br>
- Evaluar tests consistentes en diferentes escenarios de procesamiento, variando la herramienta usada, el volumen de los datos y la cantidad de variables auxiliares creadas. <br>
:::


## Herramientas para datos grandes

. . .

![](imagenes/logo_apache_arrow.png){.absolute top=150 left=100 width="170" height="150"}
![](imagenes/github_arrow_01.png){.absolute top=100 left=100 width="350" height="50"}
![](imagenes/github_arrow_02.png){.absolute top=150 left=300 width="150" height="200"}

. . .

![](imagenes/logo_apache_spark.png){.absolute top=150 left=600 width="170" height="150"}
![](imagenes/github_spark_01.png){.absolute top=100 left=600 width="350" height="50"}
![](imagenes/github_spark_02.png){.absolute top=150 left=800 width="150" height="200"}

. . .

![](imagenes/logo_duckdb.png){.absolute top=450 left=100 width="200" height="150"}
![](imagenes/github_duckdb_01.png){.absolute top=400 left=100 width="350" height="50"}
![](imagenes/github_duckdb_02.png){.absolute top=450 left=300 width="150" height="200"}

. . .

![](imagenes/logo_datatable.png){.absolute top=450 left=600 width="200" height="200"}
![](imagenes/github_datatable_01.png){.absolute top=400 left=600 width="350" height="50"}
![](imagenes/github_datatable_02.png){.absolute top=450 left=800 width="150" height="200"}


## Herramientas para datos grandes

**Apache Arrow:**

<!-- ![](imagenes/logo_apache_arrow.png){.absolute top=60 left=200 width="70" height="50"} -->

::: columns

:::{.column width="50%" .incremental .small-par}

- Su formato columnar *in-memory* y agnóstico al lenguaje permite representar datasets tabulares estructurados en memoria. <br><br>
- Lectura y escritura en muchos formatos comunes de almacenamiento (csv, parquet, feather, etc.). <br><br>
- Tiene implementaciones en muchos lenguajes (C++, Java, Python, R, etc.). <br><br>
- Implementación en R permite ejecutar operaciones analíticas o queries sobre objectos *Table* and *Dataset* con librería *dplyr*.  

:::

:::{.column width="50%" .incremental .small-par}

![](imagenes/arrow_columnar_format.png){width="90%"}

:::

:::


## Herramientas para datos grandes

**Apache Spark:**

<!-- ![](imagenes/logo_apache_spark.png){.absolute top=60 left=200 width="70" height="50"} -->

::: columns

:::{.column width="40%" .incremental .small-par}

- Framework para el procesamiento distribuido de grandes datos. <br><br>
- Procesamiento escala horizontalmente y se distribuye en múltiples nodos de un cluster. <br><br>
- Procesos Spark: 
  - Coordinados por el objeto *SparkContext* en nuestro programa main (driver).
  - *SparkContext* se conecta a un *Cluster Manager* (Standalone, YARN, Kubernetes) que asigna los recursos del cluster.

:::

:::{.column width="60%" .incremental .small-par}

![](imagenes/spark_02_1.png){width="100%"}

![](imagenes/spark_01.png){width="70%"}

:::

:::


## Herramientas para datos grandes

**Apache Spark:**

<!-- ![](imagenes/logo_apache_spark.png){.absolute top=60 left=200 width="70" height="50"} -->

::: columns

:::{.column width="40%" .small-par}

- Framework para el procesamiento distribuido de grandes datos. <br><br>
- Procesamiento escala horizontalmente y se distribuye en múltiples nodos de un cluster. <br><br>
- Procesos Spark: 
  - Coordinados por el objeto *SparkContext* en nuestro programa main (driver).
  - *SparkContext* se conecta a un *Cluster Manager* (Standalone, YARN, Kubernetes) que asigna los recursos del cluster.

:::

:::{.column width="60%" .incremental .small-par}

![](imagenes/spark_02_2.png){width="100%"}

![](imagenes/spark_01.png){width="70%"}

:::

:::


## Herramientas para datos grandes

**Apache Spark:**

<!-- ![](imagenes/logo_apache_spark.png){.absolute top=60 left=200 width="70" height="50"} -->

::: columns

:::{.column width="40%" .small-par}

- Framework para el procesamiento distribuido de grandes datos. <br><br>
- Procesamiento escala horizontalmente y se distribuye en múltiples nodos de un cluster. <br><br>
- Procesos Spark: 
  - Coordinados por el objeto *SparkContext* en nuestro programa main (driver).
  - *SparkContext* se conecta a un *Cluster Manager* (Standalone, YARN, Kubernetes) que asigna los recursos del cluster.

:::

:::{.column width="60%" .incremental .small-par}

![](imagenes/spark_02_3.png){width="100%"}

![](imagenes/spark_01.png){width="70%"}

:::

:::


## Herramientas para datos grandes

**Apache Spark:**

<!-- ![](imagenes/logo_apache_spark.png){.absolute top=60 left=200 width="70" height="50"} -->

::: columns

:::{.column width="40%" .small-par}

- Framework para el procesamiento distribuido de grandes datos. <br><br>
- Procesamiento escala horizontalmente y se distribuye en múltiples nodos de un cluster. <br><br>
- Procesos Spark: 
  - Coordinados por el objeto *SparkContext* en nuestro programa main (driver).
  - *SparkContext* se conecta a un *Cluster Manager* (Standalone, YARN, Kubernetes) que asigna los recursos del cluster.

:::

:::{.column width="60%" .incremental .small-par}

![](imagenes/spark_02_4.png){width="100%"}

![](imagenes/spark_01.png){width="70%"}

:::

:::


## Herramientas para datos grandes 

**Apache Spark:**

<!-- ![](imagenes/logo_apache_spark.png){.absolute top=60 left=200 width="60" height="40"} -->


::: {.medium-par}
- Procesamiento versátil mediante varias APIs: en batches (Spark Core), queries SQL (Spark SQL), streaming (Spark Streaming), en graphs (GraphX) y machine learning (MLlib). <br><br>
- APIs de alto nivel para varios lenguages: Scala, Java, Python y R. <br><br>
- Integrable con varias fuentes de datos: Hadoop Distributed File System (HDFS), BBDD relacionales, data lakes, etc. <br><br>
- Soporta varios formatos de archivo: CSV, Parquet, Avro, JSON, etc. <br><br>
:::



## Herramientas para datos grandes 

**Duck DB:**

<!-- ![](imagenes/logo_duckdb.png){.absolute top=65 left=140 width="80" height="40"} -->

::: {.medium-par}
- Sistema de manejo de base de datos analíticas (DBMS) diseñado para reportería y analítica. <br><br>
- Almacena data en formato columnar, que está optimizado para queries analíticas. <br><br>
- Usa un motor de ejecución de queries vectorizado, que procesa múltiple elementos de data (vectores) en una única instrucción. <br><br> 
- Soporta sintaxis SQL estándard. <br><br>
- ...
:::


## Herramientas para datos grandes 

**Data Table:**

<!-- ![](imagenes/logo_datatable.png){.absolute top=60 left=140 width="80" height="60"} -->

::: {.medium-par}
- Architecture. <br><br>
- Main characteristics. <br><br>
:::


<!-- ## Caso práctico: Censo

TODO:

::: {.medium-par}
- Describe dataset. <br><br>
- Describe different tests. <br><br>
- Describe different processing stages. <br><br>
- Describe different tools used. <br><br>
::: -->


## Tests: Volumen de datos

::: {.medium-par}

<br><br>

+----------------+---------------------------------------------+
| Volumen        | Detalle                                     |
+================+=============================================+
| Sin aumentar   | Dataset original                            |
|                |                                             |
+----------------+---------------------------------------------+
| Aumentado +5M  | Dataset con 5M de registros repetidos       |
|                |                                             |
+----------------+---------------------------------------------+
| Aumentado +10M | Dataset con 10M de registros repetidos      |
|                |                                             |
+----------------+---------------------------------------------+
| Sin aumentar   | Dataset con el doble de variables creadas   |
| Variables 2X   |                                             |
+----------------+---------------------------------------------+
| Aumentado +5M  | Dataset con 5M de registros repetidos       |
| Variables 2X   | y con el doble de variables creadas         |
+----------------+---------------------------------------------+
| Aumentado 10M  | Dataset con 10M de registros repetidos      |
| Variables 2X   | y con el doble de variables creadas         |
+----------------+---------------------------------------------+

::: 


## Tests: Etapa en procesamiento

::: {.medium-par}

<br><br>

+----------------------+--------------------------------------------------------------------------+
| Etapa                | Detalle                                                                  |
+======================+==========================================================================+
| Cargar data          | Lectura de dataset en R o Python                                         |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Homologar variables  | Transformación de algunas columnas en dataset                            |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Crear indices        | Creación de índices para columnas de agrupación (sólo para Data table)   |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Crear variables      | Creación de variables auxiliares de validación                           |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Merge variables      | Consolidación de variables de validación creadas                         |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Left join            | Unión de variables de validación a dataset                               |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+
| Exportar data        | Guardado de dataset transformado                                         |
|                      |                                                                          |
+----------------------+--------------------------------------------------------------------------+

::: 


## Tests: Herramienta de procesamiento

::: {.medium-par}

<br><br>

+----------------------+------------------------------------------------------------------+
| Herramienta          | Detalle                                                          |
+======================+==================================================================+
| Apache Arrow (R)     | Implementación de Arrow en R (operaciones usando *dplyr*)        |
|                      |                                                                  |
+----------------------+------------------------------------------------------------------+
| Apache Arrow         | Implementación de Arrow en R (operaciones usando *dplyr*).       | 
| + DuckDB (R)         | Algunas operaciones ejecutadas usando DuckDB                     |
+----------------------+------------------------------------------------------------------+
| Apache Arrow         | Implementación de Arrow en Python (PyArrow)                      |
| + (Python)           |                                                                  |
+----------------------+------------------------------------------------------------------+
| Data Table (R)       | Librería Data.table de R                                         |
|                      |                                                                  |
+----------------------+------------------------------------------------------------------+
| Apache Spark (R)     | Implementación de Spark en R (librería *sparklyr*)               |
|                      |                                                                  |
+----------------------+------------------------------------------------------------------+

::: 


## Código procesamiento: Cargar data

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.feather"
persona <- arrow::read_feather(
  filename,
  as_data_frame = F,
  col_select=columnas_selected
)

cols_partition <- c("REGION","PROVINCIA","COMUNA")
format_dataset <- "feather"

path_dataset <- paste0(ubi,"test_arrow_dataset_in_tmp")
arrow::write_dataset(persona, path_dataset, format=format_dataset, partitioning=cols_partition)
persona <- arrow::open_dataset(path_dataset, format=format_dataset, partitioning=cols_partition)
```

### [Apache Arrow (Python)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE

filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.feather"
persona = ft.read_table(filename) # result is pyarrow.Table
# persona = ft.read_feather(filename) # result is pandas.DataFrame

# create dataset
path_dataset = f"{ubi}test_pyarrow_dataset_in_tmp"
partitioning = ["REGION","PROVINCIA"]
ds.write_dataset(
    persona,
    base_dir=path_dataset,
    format="feather",
    partitioning=partitioning,
    existing_data_behavior="delete_matching"
)
```

### [Data Table (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.csv"
persona <- data.table::fread(
  filename,
  select=list(integer=columnas_integer, character=columnas_character)
)
```

### [Apache Spark (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename <- "Microdato_Censo2017-Personas__data_no_aumentado_arrow_withnrow.feather"
persona <- spark_read_csv(
  sc,
  name="persona",
  path=filename,
  header=TRUE,
  columns=columnas,
  infer_schema=FALSE,
  delimiter=';',
  repartition=0,
  memory=FALSE
) %>%
select(all_of(columnas_2))
```

:::


## Código procesamiento: Homologar variables

::: {.medium-par}
- Code example. <br><br>
:::


## Código procesamiento: Crear índices

::: {.medium-par}
- Code example. <br><br>
:::


## Código procesamiento: Crear variables

::: {.medium-par}
- Code example. <br><br>
:::


## Código procesamiento: Merge variables

::: {.medium-par}
- Code example. <br><br>
:::


## Código procesamiento: Left join

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.medium-par} 

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- persona %>%
  left_join(edad_hhpmss_jefe_hogar, by = c("entrevista_id","rst_hogares__id")) %>%
  compute()
```

### [Apache Arrow (Python)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
ks = ["entrevista_id","rst_hogares__id"]

persona = persona\
    .join(edad_hhpmss_jefe_hogar, keys=ks)
```

### [Data Table (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- merge(
  persona,
  edad_hhpmss_jefe_hogar,
  by=c("entrevista_id","rst_hogares__id"),
  all.x=TRUE,
  all.y=FALSE
)
```

### [Apache Spark (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
persona <- persona %>%
  left_join(edad_hhpmss_jefe_hogar, by = c("entrevista_id","rst_hogares__id"))
```

:::


## Código procesamiento: Exportar data

::: {.panel-tabset}

### [Apache Arrow (R) [+DuckDB]]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
format_dataset <- "feather"
path_dataset <- "test_arrow_dataset_out"

arrow::write_dataset(persona, path_dataset, format=format_dataset, partitioning=cols_partition)
```

### [Apache Arrow (Python)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE

partitioning = ["REGION","PROVINCIA"]
path_dataset = "test_pyarrow_dataset_out"

ds.write_dataset(
    persona,
    base_dir=path_dataset,
    format="feather",
    partitioning=partitioning,
    existing_data_behavior="delete_matching"
)
```

### [Data Table (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
filename_out <- "integrada_persona____data_no_aumentado.feather"

feather::write_feather(persona, filename_out)
```

### [Apache Spark (R)]{.medium-par}

```{r}
#| echo: TRUE
#| eval: FALSE
dir_out <- "integrada_persona_csv__data_no_aumentado"

sparklyr::spark_write_csv(
  persona,
  path=dir_out,
  header = TRUE,
  delimiter = ";",
  quote = "\"",
  escape = "\\",
  charset = "UTF-8",
  null_value = NULL,
  options = list(),
  mode = "overwrite"
)
sparklyr::spark_disconnect(sc)
```

:::


## Resultados: Data sin aumentar

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_no_aum.png" width="100%"/>  


## Resultados: Data aumentado +5M

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_05M.png" width="100%"/>  


## Resultados: Data aumentado +10M

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_10M.png" width="100%"/>  


## Resultados: Data sin aumentar + variables 2X

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_no_aum_var_aum1X.png" width="100%"/>  


## Resultados: Data aumentado +5M + variables 2X

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_05M_var_aum1X.png" width="100%"/>  


## Resultados: Data aumentado +10M + variables 2X

<img src="imagenes/test_eficiencia_censo_datatable_vs_spark_vs_arrow_vs_arrowdb_vs_pyarrow_data_aum_10M_var_aum1X.png" width="100%"/>  



## Conclusión

::: {.medium-par}

- Some observations on test results.

- Best test ?

:::


## Referencias

TODO: Agregar referencias a documentación de herramientas / algoritmos

::: {.medium-par}

- [R for Data Science, de Hadley Wickham](https://r4ds.had.co.nz/)

:::

#

<!---
# TODO: this does not work
 .linea-superior[]
.linea-inferior[] 
--->

<!---
# TODO: this does not work
 ![](imagenes/logo_portada2.png){.center style="width: 20%;"}   
--->

<img src="imagenes/logo_portada2.png" width="20%"/>  

[**Grandes Volúmenes de Datos**]{.big-par .center-justified}

[**Proyecto Ciencia de Datos**]{.big-par .center-justified}

[**Octubre 2023**]{.big-par .center-justified}

[]{.linea-superior} 
[]{.linea-inferior} 

<!---
 ## Polar Axis  

For a demonstration of a line plot on a polar axis, see @fig-polar.

```{python}
#| label: fig-polar
#| fig-cap: "A line plot on a polar axis, not a bear"

import numpy as np
import matplotlib.pyplot as plt

r = np.arange(0, 2, 0.01)
theta = 4 * np.pi * r
fig, ax = plt.subplots(
  subplot_kw = {'projection': 'polar'} 
)
ax.plot(theta, r)
ax.set_rticks([0.5, 1, 1.5, 2])
ax.grid(True)
plt.show()
``` 
--->